# language-model-from-scratch
This repo implements a small GPT-style language model end-to-end from scratch. I wrote the byte-level BPE tokenizer, AdamW optimizer, and Transformer (Multi-Head Self-Attention, Pre-Norm blocks, RoPE) without PyTorch convenience modules
